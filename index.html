<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robotic Trash Collection Research</title>
    <style>
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            /* max-width: 800px; */
            margin: 0 auto;
            padding: 2rem;
            color: #333;
            background-color: #f5f5f5;
        }

        .container {
            background-color: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        h1 {
            color: #2563eb;
            margin-bottom: 1.5rem;
            font-size: 2.25rem;
            text-align: center;
        }

        h2 {
            color: #1e40af;
            /* margin-top: 2rem;
            margin-bottom: 1rem; */
            margin-top:1.5rem;
            margin-bottom: 0.5rem;
            font-size: 1.5rem;
        }

        h3 {
            color: #112771;
            margin-top: 0.05rem;
            margin-bottom: 0.05rem;
            font-size: 1rem;
            text-align: center;
        }

        .timeline {
            border-left: 3px solid #2563eb;
            padding-left: 1.5rem;
            margin: 2rem 0;
        }

        .timeline-item {
            margin-bottom: 1.5rem;
            position: relative;
        }

        .timeline-item::before {
            content: '';
            position: absolute;
            left: -1.7rem;
            top: 0.5rem;
            width: 12px;
            height: 12px;
            background-color: #2563eb;
            border-radius: 50%;
        }

        .section {
            margin-bottom: 2rem;
        }

        .highlight {
            background-color: #f0f9ff;
            padding: 1rem;
            border-radius: 4px;
            border-left: 4px solid #2563eb;
            margin: 1rem 0;
        }

        .insight {
            font-style: italic;
            color: #4b5563;
            margin: 0.5rem 0;
        }
        
        @media (max-width: 50em) {
            .img-container {
                width: 100%;
                padding: 56.25%;
            }
            .img {
                width: 100%;
                height: 100%;
            }
            
            .video-container {
                position: relative;
                width: 100%;
                padding-bottom: 56.25%; /* 16:9 aspect ratio */
                height: 0;
            }

            .video-container iframe {
                position: absolute;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
            }
	    }
    </style>
</head>
<body>
    <div class="container">
        <h1>Autonomous Robotic Trash Collection: Context-Aware Detection and Control</h1>

        <h3>Joe Lisk</h3>
        <h3>University of Minnesota</h3>
        <h3>M.S. Robotics</h3>
        <h3>ROB 8760: Capstone Project</h3>
        <h3>Advisor: Nikolaos Papanikolopoulos</h3>
        <h3>December 2024</h3>

        <h2>Abstract</h2>
        <div class="section">
            <p>
                This work demonstrated a proof-of-concept for utilizing prompt caching and feature-rich trash 
                data for autonomous trash collection. Prompt caching is a technique in which exact or similar 
                prompts are compared to access existing solutions from a large language model (LLM), which 
                can be used with robotic systems. Prompt caching can be helpful when using LLMs in robotic 
                systems by saving on the number of prompts for repeated tasks and reducing the uncertainty 
                associated with the LLM's output. Likewise, it was discovered that using additional descriptors 
                for objects can assist LLMs in determining which objects in a scene are trash. This work was 
                built upon previous iterations of robotic trash collection. In previous iterations, GPT-4 was 
                tasked with determining which objects in a scene were trash and providing code to actuate a 
                Kinova Gen3 Lite robotic arm to pick up and dispose of the trash. 
            </p>
        </div>

        <h2>Research Journey</h2>
        <div class="timeline">
            <div class="timeline-item">
                <strong>Fall 2023:</strong> Used ChatGPT for binary trash classification and Kinova Gen3 Lite arm control
                <div class="insight">This initial phase explored the feasibility of using LLMs for both object classification and robotic control, setting the foundation for understanding the challenges in autonomous trash collection.</div>
                
            </div>
            <div class="timeline-item">
                <strong>Spring 2024:</strong> Evaluated YOLOv9 object detection and fine-tuned BLIP-2 VLM on TACO dataset
                <div class="insight">This phase aimed to benchmark state-of-the-art computer vision approaches against the unique challenges of trash detection.</div>
            </div>
        </div>

        <div class="img-container">
            <img height="296" width="277" src="https://joelisk.github.io/umn-f24-rob8760-capstone-project/assets/vlmtrash.png" alt="vlm captions trash">
        </div>
        <div class="img-container">
            <img height="241" width="319" src="https://joelisk.github.io/umn-f24-rob8760-capstone-project/assets/clutterROSYOLO.png" alt="yolo trash detection">
        </div>
        <div class="insight">left: fine-tuned BLIP-2 caption on TACO dataset. right: YOLOv9 trash detection results in cluttered scene</div>
        <div class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/h89XDdHrP6c?si=EV7wqUHLNH5jUEQD" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
        <div class="insight">video: GPT-4 based control of Kinova Gen 3 Lite</div>

        <h2>Key Discoveries & Motivations</h2>
        <div class="highlight">
            <p>Through these early experiments, two critical insights emerged:</p>
            <ol>
                <li>Object detection alone proved insufficient due to trash's subjective, context-dependent nature - an aspect overlooked in existing research</li>
                <li>While LLMs showed promise for robotic control, they introduced significant cost and risk factors that needed mitigation</li>
            </ol>
        </div>

        <h2>Capstone Focus & Innovation</h2>
        <div class="section">
            <p>These discoveries led to two novel research directions:</p>
            <ol>
                <li>Exploring the addition of scene context in public spaces to improve trash detection accuracy
                    <div class="insight">This approach was motivated by the observation that context is crucial for human trash identification, yet no existing works had explored this avenue for autonomous systems.</div>
                </li>
                <li>Implementing prompt caching via embedding similarity to optimize LLM usage
                    <div class="insight">This innovation addressed the practical challenges of LLM integration, offering a way to maintain performance while reducing operational costs and risks.</div>
                </li>
            </ol>
        </div>

        <h2>Methodology</h2>
        <div class="section">
            <p>The research employed two complementary approaches:</p>
            <ol>
                <li>Synthetic Data Generation
                    <div class="insight">Used GPT to create crowded airport scenarios due to the current lack of contextually-rich trash detection datasets. This approach allowed for controlled testing of context-based classification.</div>
                </li>
                <li>BERT-based Similarity Analysis
                    <div class="insight">Implemented command similarity detection to enable efficient caching of robot control solutions, reducing the need for repeated LLM queries.</div>
                </li>
            </ol>
        </div>

        <h2>Results</h2>
        <div class="highlight">
            <p>The research yielded two significant findings:</p>
            <ol>
                <li>Additional scene context demonstrably improved trash classification accuracy of detected objects</li>
                <li>Command caching with a 0.3 similarity threshold effectively reduced LLM query requirements while maintaining system reliability</li>
            </ol>
        </div>

        <div class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/ExTagq-P8fI?si=KbLH6Skt18BE0VZN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
        <div class="insight">video: LLM-assisted human-robot interaction for trash classification</div>
        <div class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/KqLIScFPiAA?si=xFhOV0c7be8-4jnY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
        <div class="insight">video: prompt caching using embedding similarity</div>

        <h2>Limitations & Future Work</h2>
        <div class="section">
            <p>Several important considerations emerged:</p>
            <ol>
                <li>While synthetic data provided valuable insights, a real-world dataset incorporating contextual information needs to be developed</li>
                <li>Geometric variations in scenes may affect the reliability of cached action sequences, though this can be mitigated through robust collision avoidance</li>
                <li>Storage optimization for accumulated tasks requires further investigation</li>
            </ol>
        </div>
    </div>
</body>
</html>
